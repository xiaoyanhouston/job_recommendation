{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaoyanhouston/job_recommendation/blob/main/job_recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8e1gAxCpW5k",
        "outputId": "d13b0cb1-e17b-4250-a456-f0078d5c5e6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "MOUNTPOINT = '/content/drive'\n",
        "drive.mount(MOUNTPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUfPx0cqrVz5",
        "outputId": "a5fbba93-32e7-4025-d7ba-7c1aa0d2c183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv langchain openai pypdf pymupdf  tiktoken chromadb PyPDF2 langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvGqPESLpwPS",
        "outputId": "31982c79-ddab-4d83-a9e1-8c9c40a0d20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.21.2-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.9/309.9 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.2-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb\n",
            "  Downloading chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.3-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
            "  Downloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.42 (from langchain)\n",
            "  Downloading langchain_core-0.1.44-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.48-py3-none-any.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Collecting PyMuPDFb==1.24.1 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.1)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.24.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.24.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.24.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.0)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.62.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.4)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson>=3.9.12 (from chromadb)\n",
            "  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Collecting packaging>=19.1 (from build>=1.0.3->chromadb)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting importlib-metadata<=7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.63.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.24.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.24.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.24.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.45b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.18.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53724 sha256=92cbeb6291ecd6b3bd94cf1710f8fc4974296e6b18592543860f0fe8b56f98d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, python-dotenv, PyPDF2, pypdf, PyMuPDFb, pulsar-client, packaging, overrides, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, jsonpointer, importlib-metadata, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, pymupdf, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, marshmallow, jsonpatch, httpcore, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, langchain-core, opentelemetry-instrumentation-fastapi, langchain-text-splitters, langchain-openai, langchain-community, langchain, chromadb\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 7.1.0\n",
            "    Uninstalling importlib_metadata-7.1.0:\n",
            "      Successfully uninstalled importlib_metadata-7.1.0\n",
            "Successfully installed PyMuPDFb-1.24.1 PyPDF2-3.0.1 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.24 coloredlogs-15.0.1 dataclasses-json-0.6.4 deprecated-1.2.14 fastapi-0.110.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 humanfriendly-10.0 importlib-metadata-7.0.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-29.0.0 langchain-0.1.16 langchain-community-0.0.33 langchain-core-0.1.44 langchain-openai-0.1.3 langchain-text-splitters-0.0.1 langsmith-0.1.48 marshmallow-3.21.1 mmh3-4.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.17.3 openai-1.21.2 opentelemetry-api-1.24.0 opentelemetry-exporter-otlp-proto-common-1.24.0 opentelemetry-exporter-otlp-proto-grpc-1.24.0 opentelemetry-instrumentation-0.45b0 opentelemetry-instrumentation-asgi-0.45b0 opentelemetry-instrumentation-fastapi-0.45b0 opentelemetry-proto-1.24.0 opentelemetry-sdk-1.24.0 opentelemetry-semantic-conventions-0.45b0 opentelemetry-util-http-0.45b0 orjson-3.10.1 overrides-7.7.0 packaging-23.2 posthog-3.5.0 pulsar-client-3.5.0 pymupdf-1.24.2 pypdf-4.2.0 pypika-0.48.9 python-dotenv-1.0.1 starlette-0.37.2 tiktoken-0.6.0 typing-inspect-0.9.0 uvicorn-0.29.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/My_project/Astoria_Pro\""
      ],
      "metadata": {
        "id": "MO6MteXUpwAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA,LLMChain,SimpleSequentialChain,SequentialChain\n",
        "import textwrap\n",
        "import os\n",
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "# from openai import OpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wy3rwstqpxUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys for OPENAI\n",
        "load_dotenv('/content/drive/MyDrive/My_project/my_key/.env')\n",
        "LLM_KEY=os.getenv(\"OPENAI_API_KEY\")\n"
      ],
      "metadata": {
        "id": "4D4dZmWcwaZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test turbo 3.5 and results are not reasonable\n"
      ],
      "metadata": {
        "id": "0ql70NcYPpm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    api_key=LLM_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "vkG92cl9Jswg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test match case"
      ],
      "metadata": {
        "id": "vHV3SqfbJ2wA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reloader = PyPDFLoader(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "retext = reloader.load_and_split()\n",
        "jbloader = PyPDFLoader(f\"{file_path}/jobs/006JD.pdf\")\n",
        "jobtext = jbloader.load_and_split()"
      ],
      "metadata": {
        "id": "U55OIT-8Jspi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "{resume}, be sure include years of experience, education, skills and summarize within 200 words\"\"\"\n",
        "resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                     input_variables=[\"resume\"] )\n",
        "\n",
        "resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "job_template=\"\"\"Write a summary of the job {job} only if it matches the resume {res_sum},\n",
        "if matches, be sure to include job title, company name, Location, salarly,technical skills,\n",
        " minimum requirements and summarize the job part relevant to the resume,\n",
        "\n",
        "Title:\n",
        "Company Name:\n",
        "Location:\n",
        "Salary:\n",
        "Technical skills:\n",
        "Requirement:\n",
        "Job Summary:\n",
        "\n",
        "if the job doesn't match the resume, skip all the details mentioned above,\n",
        "just say the job does not match your experience , do not come up answer.\n",
        "\n",
        "Be sure format result itermized as HTML so it could be displayed on website.\n",
        "\"\"\"\n",
        "job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                     input_variables=[\"job\",\"res_sum\"] )\n",
        "\n",
        "job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\"])"
      ],
      "metadata": {
        "id": "1nm2Z9pcJsi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(ss_chain.run(resume=retext,job=jobtext)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "Zs15MXOkJsEP",
        "outputId": "67008068-a666-42a5-a704-b9b6c28c758d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<ul>\n",
              "<li><strong>Title:</strong> Sr Data Analyst</li>\n",
              "<li><strong>Company Name:</strong> Cribl</li>\n",
              "<li><strong>Location:</strong> United States (Remote)</li>\n",
              "<li><strong>Salary:</strong> $125,000 - $160,000 per year</li>\n",
              "<li><strong>Technical Skills:</strong> R, Python/PySpark, SQL, BigQuery, Google Cloud (GCP), AWS, Azure Databricks, Tableau, PowerBI, GitHub, Microsoft Office Suite</li>\n",
              "<li><strong>Requirement:</strong> Master's degree in Data Science, 4 years of experience working with product data, familiarity with modern data tech stack and BI tools, strong communication and organizational abilities</li>\n",
              "<li><strong>Job Summary:</strong> As a Sr Data Analyst at Cribl, you will be responsible for empowering data-driven decision-making, scaling the business intelligence system, and championing data best practices. You will collaborate with cross-functional stakeholders, translate business needs into key metrics, automate reporting needs, manage data sets, and lead the enablement of the centralized BI tool. The role requires strong analytical skills, technical expertise, and the ability to work effectively in a collaborative environment.</li>\n",
              "</ul>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test non match case"
      ],
      "metadata": {
        "id": "sDjz5GXdKGbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reloader = PyPDFLoader(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "retext = reloader.load_and_split()\n",
        "jbloader = PyPDFLoader(f\"{file_path}/jobs/001JD.pdf\")\n",
        "jobtext = jbloader.load_and_split()"
      ],
      "metadata": {
        "id": "cZ8FXFcFJr9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "{resume}, be sure include years of experience, education, skills and summarize within 200 words\"\"\"\n",
        "resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                     input_variables=[\"resume\"] )\n",
        "\n",
        "resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "job_template=\"\"\"Write a summary of the job {job} only if it matches the resume {res_sum},\n",
        "if matches, be sure to include job title, company name, Location, salarly,technical skills,\n",
        " minimum requirements and summarize the job part relevant to the resume,\n",
        "\n",
        "Title:\n",
        "Company Name:\n",
        "Location:\n",
        "Salary:\n",
        "Technical skills:\n",
        "Requirement:\n",
        "Job Summary:\n",
        "\n",
        "if the job doesn't match the resume, skip all the details mentioned above,\n",
        "just say the job does not match your experience , do not come up answer.\n",
        "\n",
        "Be sure format result itermized as HTML so it could be displayed on website.\n",
        "\"\"\"\n",
        "job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                     input_variables=[\"job\",\"res_sum\"] )\n",
        "\n",
        "job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\"])"
      ],
      "metadata": {
        "id": "YzuQwQ57Jr12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(ss_chain.run(resume=retext,job=jobtext)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ICQxYq6qJrtt",
        "outputId": "efa9a595-6a0a-4a5f-9af3-299a52182db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<ul>\n",
              "<li><strong>Title:</strong> Nurse Practitioner - Mobile Unit</li>\n",
              "<li><strong>Company Name:</strong> Third Street</li>\n",
              "<li><strong>Location:</strong> Mansfield, OH</li>\n",
              "<li><strong>Salary:</strong> $100,000</li>\n",
              "<li><strong>Technical Skills:</strong> R, Python/PySpark, SQL, BigQuery, Google Cloud (GCP), AWS, Azure Databricks, Tableau, PowerBI, GitHub, Microsoft Office Suite</li>\n",
              "<li><strong>Requirement:</strong> Current Ohio APRN license, Active DEA License, Board Certified or provide letter indicating qualification to sit for certification with expectation of obtaining certification within one year, Valid Driver’s License</li>\n",
              "<li><strong>Job Summary:</strong> Seeking an experienced family nurse practitioner to join the team and expand services via a mobile unit. Responsibilities include providing expert medical care to patients of all ages, conducting physical examinations, prescribing medications, collaborating with interdisciplinary teams, educating patients on health promotion, and maintaining accurate electronic records. The role offers competitive compensation, paid time off, and opportunities for professional development.</li>\n",
              "</ul>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test turbo 4.0"
      ],
      "metadata": {
        "id": "83ENPp-hJjIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4-turbo',\n",
        "    api_key=LLM_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "GA700ZDuDEPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test match case"
      ],
      "metadata": {
        "id": "kdVmMvr1Gbsf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "reloader = PyPDFLoader(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "retext = reloader.load_and_split()\n",
        "jbloader = PyPDFLoader(f\"{file_path}/jobs/006JD.pdf\")\n",
        "jobtext = jbloader.load_and_split()"
      ],
      "metadata": {
        "id": "1acofh8MPcUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "{resume}, be sure include years of experience, education, skills and summarize within 200 words\"\"\"\n",
        "resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                     input_variables=[\"resume\"] )\n",
        "\n",
        "resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "job_template=\"\"\"Write a summary of the job {job} only if it matches the resume {res_sum},\n",
        "if matches, be sure to include job title, company name, Location, salarly,technical skills,\n",
        " minimum requirements and summarize the job part relevant to the resume,\n",
        "\n",
        "Title:\n",
        "Company Name:\n",
        "Location:\n",
        "Salary:\n",
        "Technical skills:\n",
        "Requirement:\n",
        "Job Summary:\n",
        "\n",
        "if the job doesn't match the resume, skip all the details mentioned above,\n",
        "just say the job does not match your experience , do not come up answer.\n",
        "\n",
        "Be sure format result itermized as HTML so it could be displayed on website.\n",
        "\"\"\"\n",
        "job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                     input_variables=[\"job\",\"res_sum\"] )\n",
        "\n",
        "job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\"])"
      ],
      "metadata": {
        "id": "w2jwO4T5VsDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(ss_chain.run(resume=retext,job=jobtext)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "IaG5loTzCzWI",
        "outputId": "245a4baa-6697-4551-afed-12fa97d1a392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "```html\n",
              "<ul>\n",
              "    <li><strong>Title:</strong> Sr Data Analyst</li>\n",
              "    <li><strong>Company Name:</strong> Cribl</li>\n",
              "    <li><strong>Location:</strong> United States (Remote)</li>\n",
              "    <li><strong>Salary:</strong> $125,000 - $160,000 per year</li>\n",
              "    <li><strong>Technical skills:</strong> SQL, data modeling, building dashboards, familiarity with modern data tech stack (Snowflake, dbt), BI tools (Looker, Tableau), Python</li>\n",
              "    <li><strong>Requirement:</strong> Four years of experience with product data, proven ability to derive insights and create data visualizations, strong communication and organizational abilities, analytical thinker, comfortable with ambiguity, excels in collaborative environments.</li>\n",
              "    <li><strong>Job Summary:</strong> As a Sr Data Analyst at Cribl, you will empower data-driven decision-making across the company by scaling the business intelligence system, democratizing data use, and championing data best practices. You will collaborate with cross-functional stakeholders to understand business needs, maintain and develop automated reporting, and manage data sets into the data warehouse. This role involves both ad-hoc data requests and project work, requiring a proactive approach and high degree of ownership.</li>\n",
              "</ul>\n",
              "```"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test non match case\n"
      ],
      "metadata": {
        "id": "QL9LumhXGPnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "reloader = PyPDFLoader(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "retext = reloader.load_and_split()\n",
        "jbloader = PyPDFLoader(f\"{file_path}/jobs/001JD.pdf\")\n",
        "jobtext = jbloader.load_and_split()"
      ],
      "metadata": {
        "id": "vrE-JU-IGOwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "{resume}, be sure include years of experience, education, skills and summarize within 200 words\"\"\"\n",
        "resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                     input_variables=[\"resume\"] )\n",
        "\n",
        "resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "job_template=\"\"\"Write a summary of the job {job} only if it matches the resume {res_sum},\n",
        "if matches, be sure to include job title, company name, Location, salarly,technical skills,\n",
        " minimum requirements and summarize the job part relevant to the resume,\n",
        "\n",
        "Title:\n",
        "Company Name:\n",
        "Location:\n",
        "Salary:\n",
        "Technical skills:\n",
        "Requirement:\n",
        "Job Summary:\n",
        "\n",
        "if the job doesn't match the resume, skip all the details mentioned above,\n",
        "just say the job does not match your experience , do not come up answer.\n",
        "\n",
        "Be sure format result itermized as HTML so it could be displayed on website.\n",
        "\"\"\"\n",
        "job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                     input_variables=[\"job\",\"res_sum\"] )\n",
        "\n",
        "job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\"])"
      ],
      "metadata": {
        "id": "rpjTwL2lGOfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(ss_chain.run(resume=retext,job=jobtext)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "kBcAq--OGOW0",
        "outputId": "93554b2e-a6ba-439a-b4ee-10227628cc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The job does not match your experience."
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define a function to do the summarization for all jobs"
      ],
      "metadata": {
        "id": "WdwYcIhNLTjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4-turbo',\n",
        "    api_key=LLM_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "CrnX8tYjPgLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def job_summary(llm,resume_path,job_path):\n",
        "  reloader = PyPDFLoader(resume_path)\n",
        "  retext = reloader.load_and_split()\n",
        "  jbloader = PyPDFLoader(job_path)\n",
        "  jobtext = jbloader.load_and_split()\n",
        "  resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "  {resume}, be sure include years of experience, education, skills and summarize within 200 words\"\"\"\n",
        "  resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                      input_variables=[\"resume\"] )\n",
        "\n",
        "  resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "  job_template=\"\"\"Write a summary of the job {job} only if it matches the resume {res_sum},\n",
        "  if matches, be sure to include job title, company name, Location, salarly,technical skills,\n",
        "  minimum requirements and summarize the job part relevant to the resume,\n",
        "\n",
        "  Title:\n",
        "  Company Name:\n",
        "  Location:\n",
        "  Salary:\n",
        "  Technical skills:\n",
        "  Requirement:\n",
        "  Job Summary:\n",
        "\n",
        "  if the job doesn't match the resume, skip all the details mentioned above,\n",
        "  just say the job does not match your experience , do not come up answer.\n",
        "\n",
        "  Be sure format result itermized as HTML so it could be displayed on website.\n",
        "  \"\"\"\n",
        "  job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                      input_variables=[\"job\",\"res_sum\"] )\n",
        "\n",
        "  job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "  ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\"])\n",
        "  display(HTML(ss_chain.run(resume=retext,job=jobtext)))"
      ],
      "metadata": {
        "id": "qU76ybkwGOOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the folder path\n",
        "folder_path = f\"{file_path}/jobs\"\n",
        "\n",
        "# Get a list of all files and directories in the specified folder\n",
        "files_and_dirs = os.listdir(folder_path)\n",
        "\n",
        "# Filter out only the files from the list\n",
        "files = [os.path.join(folder_path, f) for f in files_and_dirs]\n"
      ],
      "metadata": {
        "id": "cHZUA7T7GN7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res_path=f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\"\n",
        "\n",
        "for i in files:\n",
        "  print(i)\n",
        "  job_summary(llm=llm, resume_path=res_path ,job_path=i)\n",
        "  print(\"----------------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AsXj3zAHRg0q",
        "outputId": "b68f6719-e691-401c-de4d-74108b6d31fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/001JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The job does not match your experience."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/002JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The job does not match your experience."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/003JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The job does not match your experience."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/004JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "```html\n",
              "<ul>\n",
              "  <li><strong>Title:</strong> Business Analyst</li>\n",
              "  <li><strong>Company Name:</strong> Amazon.com Services LLC</li>\n",
              "  <li><strong>Location:</strong> Bellevue, WA, USA</li>\n",
              "  <li><strong>Salary:</strong> $59,500/year to $143,100/year</li>\n",
              "  <li><strong>Technical skills:</strong> Excel, SQL, R, Ruby, Python</li>\n",
              "  <li><strong>Requirement:</strong>\n",
              "    <ul>\n",
              "      <li>3+ years of tax, finance or a related analytical field experience</li>\n",
              "      <li>Bachelor's degree or equivalent</li>\n",
              "      <li>Experience with Excel and SQL</li>\n",
              "      <li>Preferred: Master's degree in business or analytical discipline, 5+ years of business analyst, data analyst or similar role experience, Advanced knowledge in scripting with R, Ruby or Python</li>\n",
              "    </ul>\n",
              "  </li>\n",
              "  <li><strong>Job Summary:</strong> As a Business Analyst at Amazon, you will develop solutions to drive insights and optimization for Network Forecasting and Scheduling. This role involves complex data analysis to identify opportunities to reduce fulfillment costs and improve efficiencies and customer experience. You will design, develop, and establish KPIs to provide strategic insights to drive growth and performance. The position requires innovative thinking and a metrics-driven mindset to solve problems and deliver solutions, involving significant cross-functional work with transportation, tech, operations, and finance teams.</li>\n",
              "</ul>\n",
              "```"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/005JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "```html\n",
              "<ul>\n",
              "  <li>Title: Senior Data Scientist - Algorithms, Payments (6 Month Contract)</li>\n",
              "  <li>Company Name: Airbnb</li>\n",
              "  <li>Location: United States (Remote Eligible)</li>\n",
              "  <li>Salary: $120 — $125 USD per hour</li>\n",
              "  <li>Technical Skills: Python, SQL, machine learning models, data analysis, modern frameworks like Pytorch or Tensorflow</li>\n",
              "  <li>Requirement: Advanced degree in a quantitative field (PhD is a plus), 5+ years of relevant experience, strong fluency in Python, familiarity with SQL at scale, deep understanding of machine learning techniques</li>\n",
              "  <li>Job Summary: The role involves identifying high-impact business opportunities through data exploration and model prototyping, translating business problems into scientific formulations, and developing, productionizing, and operating machine learning models and pipelines at scale. The position focuses on enhancing Airbnb's payment experiences, improving business outcomes, and bolstering fraud detection capabilities.</li>\n",
              "</ul>\n",
              "```"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/007JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The job does not match your experience."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/006JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "```html\n",
              "<ul>\n",
              "  <li><strong>Title:</strong> Sr Data Analyst</li>\n",
              "  <li><strong>Company Name:</strong> Cribl</li>\n",
              "  <li><strong>Location:</strong> Remote, United States</li>\n",
              "  <li><strong>Salary:</strong> $125,000 - $160,000 per year</li>\n",
              "  <li><strong>Technical Skills:</strong> SQL, data modeling, BI tools (Looker, Tableau), data transformations, modern data tech stack (Snowflake, dbt)</li>\n",
              "  <li><strong>Requirement:</strong> Four years of experience with product data, strong analytical skills, proficiency in SQL and data visualization, familiarity with modern data tech stack and BI tools, strong communication and organizational abilities.</li>\n",
              "  <li><strong>Job Summary:</strong> As a Sr Data Analyst at Cribl, you will empower data-driven decision-making across the company by scaling the business intelligence system, democratizing data use, and championing data best practices. You will collaborate with cross-functional stakeholders to understand business needs, maintain and develop automated reporting, and manage data integration into the data warehouse. The role involves creating meaningful data transformations and enabling self-service data access through BI tools.</li>\n",
              "</ul>\n",
              "```"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n",
            "/content/drive/MyDrive/My_project/Astoria_Pro/jobs/008JD.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "The job does not match your experience."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xy9r8jKnRwp7",
        "outputId": "347abeb9-b64e-483d-9bcf-6d4ca66b55a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/My_project/Astoria_Pro/jobs/001JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/002JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/003JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/004JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/005JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/007JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/006JD.pdf',\n",
              " '/content/drive/MyDrive/My_project/Astoria_Pro/jobs/008JD.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assess a sepecific job for the talent"
      ],
      "metadata": {
        "id": "kUTWpcaz7j7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4-turbo',\n",
        "    api_key=LLM_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "HMwd6_QN7ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloader = PyPDFLoader(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "retext = reloader.load_and_split()\n",
        "jbloader = PyPDFLoader(f\"{file_path}/jobs/005JD.pdf\")\n",
        "jobtext = jbloader.load_and_split()"
      ],
      "metadata": {
        "id": "p08q41W28_un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "{resume}, be sure include years of experience, education, skills and summarize within 200 words\"\"\"\n",
        "resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                     input_variables=[\"resume\"] )\n",
        "\n",
        "resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "job_template=\"\"\"you are a career expert need to assess the job {job} for the candidate\n",
        "with the resume {res_sum} by the following 2 steps:\n",
        "1. summarize the part in the job description relevant to the resume\n",
        "2. provide assessment summary to the candidate where he/she need to improve\n",
        "show the results as the following 2 items,\n",
        "Relevant summary:\n",
        "Need to improve:\n",
        "\n",
        "Be sure format result itermized as HTML so it could be displayed on website.\n",
        "\"\"\"\n",
        "job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                     input_variables=[\"job\",\"res_sum\"] )\n",
        "\n",
        "job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\"])"
      ],
      "metadata": {
        "id": "d1njuIhY7ikB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(ss_chain.run(resume=retext,job=jobtext)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "wv3mU95Q7iQt",
        "outputId": "56c9cd70-d4b7-48f1-b90a-e8040cc2a894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "```html\n",
              "<ol>\n",
              "    <li>\n",
              "        <strong>Relevant Summary:</strong>\n",
              "        <p>The Senior Data Scientist role at Airbnb involves working within the Payments team to develop scalable machine learning models and data-driven solutions to enhance payment experiences, improve business outcomes, and bolster fraud detection capabilities. The role requires advanced skills in Python, SQL, and familiarity with machine learning frameworks like Pytorch or TensorFlow. The candidate will engage in hands-on development and operation of machine learning models, both in batch and real-time scenarios, and collaborate with cross-functional teams to drive scientific decisions and quantify impacts. The position is remote but requires residency in specific states where Airbnb has a registered entity.</p>\n",
              "    </li>\n",
              "    <li>\n",
              "        <strong>Need to Improve:</strong>\n",
              "        <p>Stella Wu appears to have a strong background in data science with relevant technical skills and project experience. However, to enhance her candidacy for the Senior Data Scientist role at Airbnb, she could focus on gaining more direct experience with real-time machine learning model productionization, as this is highlighted as a plus in the job description. Additionally, while she has experience in payments from a previous role, further emphasizing her knowledge and experience in fraud detection or risk management within the payments sector could strengthen her application. Engaging with the scientific community through publications, as mentioned in the job description, could also improve her standing and visibility in the field.</p>\n",
              "    </li>\n",
              "</ol>\n",
              "```"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YriTDpiU9dgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a cover letter"
      ],
      "metadata": {
        "id": "4bTJ1fe6BqIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4-turbo',\n",
        "    api_key=LLM_KEY\n",
        ")"
      ],
      "metadata": {
        "id": "JrTTXlHz9dcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloader = PyPDFLoader(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "retext = reloader.load_and_split()\n",
        "jbloader = PyPDFLoader(f\"{file_path}/jobs/005JD.pdf\")\n",
        "jobtext = jbloader.load_and_split()"
      ],
      "metadata": {
        "id": "pBeI-50m9dYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "today= date.today()"
      ],
      "metadata": {
        "id": "MTsbzlLUZ6ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_template=\"\"\"Write a comprehensive summary of the resume in:\n",
        "{resume}, be sure include years of experience, education, skills and summarize within 400 words\"\"\"\n",
        "resume_prompt_template=PromptTemplate(template=resume_template,\n",
        "                                     input_variables=[\"resume\"] )\n",
        "\n",
        "resume_chain = LLMChain(llm=llm, prompt=resume_prompt_template,output_key=\"res_sum\")\n",
        "job_template=\"\"\"you are a career expert need to write a cover letter for the candidate\n",
        "with the resume {res_sum} and the candidate is applying the job {job}.\n",
        "make sure to highlight the relevant experience to the job. Use 'Cover Letter' as title in the middle,\n",
        "use {today} as date to the left and  start the letter body with Dear Hiring Manager. Order them as :\n",
        "Title\n",
        "Date\n",
        "Letter Body\n",
        ".\n",
        "\n",
        "Be sure format result itermized as HTML so it could be displayed on website.\n",
        "\"\"\"\n",
        "job_prompt_template=PromptTemplate(template=job_template,\n",
        "                                     input_variables=[\"job\",\"res_sum\",\"today\"] )\n",
        "\n",
        "job_chain = LLMChain(llm=llm, prompt=job_prompt_template)\n",
        "\n",
        "ss_chain= SequentialChain(chains=[resume_chain, job_chain],input_variables=[\"resume\",\"job\",\"today\"])"
      ],
      "metadata": {
        "id": "or36731h9dTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(ss_chain.run(resume=retext,job=jobtext,today=today)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "goZjMw7hfkll",
        "outputId": "6b35b278-57c6-477c-b3ea-8adb1f92ac86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "```html\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "<head>\n",
              "<meta charset=\"UTF-8\">\n",
              "<title>Cover Letter</title>\n",
              "</head>\n",
              "<body>\n",
              "    <h1 style=\"text-align: center;\">Cover Letter</h1>\n",
              "    <p style=\"text-align: left;\">2024-04-18</p>\n",
              "    <p>Dear Hiring Manager,</p>\n",
              "    <p>I am writing to express my interest in the Senior Data Scientist - Algorithms, Payments position at Airbnb, as advertised. With a Master’s degree in Social Data Analysis and Research from the University of Texas at Dallas and over five years of professional experience in data science, I am excited about the opportunity to contribute to your team.</p>\n",
              "    <p>At my previous position with Bella Electrical Appliances, I honed my skills in predictive analytics and machine learning, utilizing SQL for data organization and applying advanced statistical techniques to inform strategic business decisions. My role involved crafting visualizations to aid stakeholder comprehension and leading hypothesis tests that drove a 30% enhancement in product quality assurance.</p>\n",
              "    <p>During my Master’s program, I completed several projects that are directly applicable to the challenges at Airbnb. For instance, I streamlined data ETL processes using Google Cloud Big Query and SQL databases for a house price prediction model, which is akin to the data-driven modeling solutions needed in your payments team. Additionally, my experience with machine learning models such as Random Forest and Gradient Boosting Tree, and my proficiency in Python/PySpark, SQL, and BigQuery align well with the technical requirements of this role.</p>\n",
              "    <p>I am particularly drawn to this role at Airbnb because of the opportunity to work on scaling payment solutions globally, which complements my background in handling large datasets and my passion for creating impactful data products. My project experience includes enhancing model performance through Natural Language Processing and conducting A/B testing on marketing data—skills that I understand are pertinent to your team’s goals.</p>\n",
              "    <p>I am enthusiastic about the possibility of bringing my unique expertise in data visualization and predictive modeling to Airbnb, and I am eager to contribute to innovative projects that enhance user experience and drive business growth. I look forward to the opportunity to discuss how my background, skills, and enthusiasms align with the goals of Airbnb.</p>\n",
              "    <p>Thank you for considering my application. I hope to bring my expertise in data science to Airbnb and contribute to your team’s success.</p>\n",
              "    <p>Sincerely,</p>\n",
              "    <p>Stella Wu</p>\n",
              "</body>\n",
              "</html>\n",
              "```"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test RAG\n"
      ],
      "metadata": {
        "id": "A4okJFMw1biS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading all the pages of the PDF file using PyPDFLoader\n",
        "text=\"\"\n",
        "#Docx2txtLoader loads the Document\n",
        "loader=DirectoryLoader(f\"{file_path}/jobs/\",\n",
        "                       glob = '*.pdf',\n",
        "                       loader_cls=PyPDFLoader)\n",
        "#Load Documents and split into chunks\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap = 100)\n",
        "text = text_splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "id": "65gSzX6s1ZMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store the job description files\n",
        "#  store the vectors into vectore store\n",
        "vec_dir = f\"{file_path}/vec\"\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=LLM_KEY)\n",
        "\n",
        "# vecdb = Chroma.from_documents(documents=text,\n",
        "#                               embedding=embeddings,\n",
        "#                               persist_directory= vec_dir)\n",
        "# # store vec to disk\n",
        "# vecdb.persist()\n",
        "# vecdb = None\n",
        "\n",
        "# read vectors from local now.\n",
        "vecdb = Chroma(persist_directory=vec_dir,\n",
        "                  embedding_function=embeddings)\n",
        "\n",
        "retriever = vecdb.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "M5KKoSgE1ZAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        num_pages = len(pdf_reader.pages)\n",
        "        for page_number in range(num_pages):\n",
        "            page = pdf_reader.pages[page_number]\n",
        "            text += page.extract_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "N-eRlKEg1Y1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = extract_text_from_pdf(f\"{file_path}/resume/Resume_Stella_HR_DS.pdf\")\n",
        "query =  \"\"\"you are a career expert, your job is to rank the matched jobs from context delimited by <ctx></ctx> for the resume below the context:\n",
        "<ctx>\n",
        "{context},\n",
        "</ctx>\n",
        "\\n\n",
        "\"\"\" + resume + \"\"\"\n",
        "\\n\n",
        "short list jobs which is good fit based on skills,education and work experience mwntioned in it?\n",
        "also provide the job company name,title,salarly,job summary,put them as python list, If there is no match, just say no match.\n",
        "{question}, be sure to put the results as HTML format so it could be used on website.\n",
        "Answer: let's think step by step\"\"\"\n",
        "prompt= PromptTemplate(\n",
        "    input_variables=['context','question'],\n",
        "    template= query\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "aLGkESWm1YqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-4-turbo',\n",
        "    api_key=LLM_KEY\n",
        ")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       retriever=retriever,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                      #  ,\n",
        "                                      #  return_source_documents=True\n",
        "                                       )\n",
        "\n",
        "def print_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "8F9NcbXe9dHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(HTML(qa_chain.run(\"give me the top 3 jobs\")))"
      ],
      "metadata": {
        "id": "T5QpkVec7iGz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "67c319b9-77c6-4bea-e51a-10bfd1d2f9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Based on the resume provided by Stella Wu, it appears that her skills and experience are primarily focused on data science, including statistical modeling, predictive analytics, machine learning, and advanced analytics techniques. She is proficient in various technologies such as R, Python/PySpark, SQL, and several cloud platforms. Her experience includes roles in data analysis and project leadership, particularly in areas requiring deep analytical skills and data-driven decision-making.\n",
              "\n",
              "Given this background, we need to match her profile with the job descriptions provided in the context. However, the context primarily discusses roles and responsibilities related to SQL Server management, database maintenance, and other IT support tasks, which do not align well with Stella's data science and analytical background.\n",
              "\n",
              "Here's the analysis:\n",
              "\n",
              "1. **SQL Server Agent, Linked Servers, and Database Maintenance Planning**: This role focuses on SQL Server management, which is not a primary area of expertise for Stella, who is more into data science and analytics.\n",
              "\n",
              "2. **Experience in configuring Transparent Data Encryption (TDE) with Safeguard**: This is a specialized IT security role, which again does not match Stella's data science-focused resume.\n",
              "\n",
              "3. **Performance monitoring at the server level**: While Stella has experience with performance monitoring, her experience is more aligned with data analytics rather than IT infrastructure.\n",
              "\n",
              "4. **SSRS, SSAS, and SSIS installation and configuration; ETL process creation and data warehousing**: Although Stella has experience with SQL and data handling, the specific technologies and focus on ETL and data warehousing are not highlighted as her core areas of expertise.\n",
              "\n",
              "5. **SQL server security, authentication modes, and SQL logins**: This is more aligned with IT security and database administration, not Stella's demonstrated data science skills.\n",
              "\n",
              "Given the mismatch between the job requirements in the context and Stella Wu's resume, it appears there is **no match** for her within the provided job descriptions. Her skills would be better suited for roles that emphasize data science, machine learning, predictive analytics, and advanced statistical analysis.\n",
              "\n",
              "Here is the HTML output indicating no suitable job match:\n",
              "\n",
              "```html\n",
              "<!DOCTYPE html>\n",
              "<html lang=\"en\">\n",
              "<head>\n",
              "    <meta charset=\"UTF-8\">\n",
              "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
              "    <title>No Suitable Job Match</title>\n",
              "</head>\n",
              "<body>\n",
              "    <h1>Job Match Results for Stella Wu</h1>\n",
              "    <p>No suitable job matches found based on the provided resume and job descriptions.</p>\n",
              "</body>\n",
              "</html>\n",
              "```"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}